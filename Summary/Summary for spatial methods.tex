%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
12pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Understanding of spatial problem for Large Datasets}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Yewen Chen}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------
\usepackage[margin=0.8in]{geometry}
\usepackage{tgbonum}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{hyperref}
\usepackage{url}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}

\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}

\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}


\begin{document}
\renewcommand{\UrlFont}{\small\tt}
\bibliographystyle{abbrvnat}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt}
%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Update step:}} % Use Output in the format of Algorithm
\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents

\listoffigures % Print the list of figures

\listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------
\newpage
\section{Gaussian random field and its challenge}
Gaussian spatial processes has been popular for decades in spatial data contexts like geostatistics where they are known as kriging, and in computer experiments where they are deployed as surrogate models or emulators. More recently, they have become a popular prediction engine in the machine learning literature. The reasons are many, but the most important are probably that: the Gaussian structure affords a large degree of analytic capability not enjoyed by other general-purpose approaches to nonparametric nonlinear modeling; and because they perform well in out-of-sample tests.

Assume there is a response or dependent variable $Y(s)$ at a generic location $s \in \boldsymbol{D} \subset \mathcal{R}^2$ along with a $p \times 1$ vector of spatially referenced predictors $\boldsymbol{X}(s)$. A spatial regression model has the form
\begin{equation}
\begin{aligned}
y(s) = \boldsymbol{\beta}\boldsymbol{X}(s) + w(s) + \epsilon(s)
\end{aligned} \label{model}
\end{equation}
where $\boldsymbol{\beta}$ is the vector of regression coefficients. The residual from the regression is decomposed into two independent parts: a spatial process, $w(s)$, modelling spatial association, and an independent process, $\epsilon(s)$, also known as the nugget effect, modelling measurement error.

The nugget effect $\epsilon(s)$ is often assumed to follow a normal distribution with variance $\tau^2$ for every location $s$.
The spatial process $w(s)$ in (\ref{model}) is often referred to as spatial random effects, capturing the effect of unmeasured or unobserved covariates with spatial pattern.

The most common specification for $w(s)$ is $w(s) \sim GP(0, \boldsymbol{C}(\cdot, \cdot))$, a zero-mean Gaussian process with a valid covariance function $\boldsymbol{C}(s, s^\prime)$. It is often reasonable to assume a constant process variance and thus we specify \
$\boldsymbol{C}(s, s^\prime) = \sigma^2 \rho (s, s^\prime; \boldsymbol{\theta})$, where $\rho (s, s^\prime; \boldsymbol{\theta})$ is a correlation function and $\boldsymbol{\theta}$ is a vector of correlation parameters which needs to be estimated from a finite number of observations, $\boldsymbol{Y} = \left(y(s_1). \cdots, y(s_n)\right)^\prime$.

Therefore, $y(s)$ follows a spatial Gaussian process, and thus we have the log-likelihood function for $(\boldsymbol{\beta}, \tau^2, \sigma^2, \boldsymbol{\theta}):$
\begin{equation}
\begin{aligned}
l(\boldsymbol{\beta}, \tau^2, \sigma^2, \boldsymbol{\theta}) \propto  - \frac{1}{2}\log |\boldsymbol{\Sigma}| - \frac{1}{2}\left(\boldsymbol{Y} - \boldsymbol{\beta}\boldsymbol{X}\right)^\prime \boldsymbol{\Sigma}^{- 1}\left(\boldsymbol{Y} - \boldsymbol{\beta}\boldsymbol{X}\right)
\end{aligned} \label{loglik}
\end{equation}
where $\boldsymbol{\Sigma} = \boldsymbol{C} + \tau^2\boldsymbol{I}.$
Then best linear unbiased prediction (BLUP) at an unobserved location $s_0$ can be obtained by the kriging
equation:
\begin{equation}
\begin{aligned}
\hat{y}(s_0) = \boldsymbol{\beta}\boldsymbol{X}(s_0) + \boldsymbol{c}_{s_0}^\prime\boldsymbol{\Sigma}^{- 1}\left(\boldsymbol{Y} - \boldsymbol{\beta}\boldsymbol{X}\right)
\end{aligned} \label{Kriging}
\end{equation}
where $\boldsymbol{c}_{s_0} = \left(\boldsymbol{C}(s_0, s_1), \dots, \boldsymbol{C}(s_0, s_n)\right)^\prime$.

However, with large or massive data, direct implementation of these statistical process, including parameter etimation by (\ref{loglik}) and interpolation by (\ref{Kriging}), becomes computationally prohibitive, since evaluating the log-likelihood in (\ref{loglik}) and solving the kriging equation (\ref{Kriging}) involve the Cholesky factorization of an $n \times n$ covariance matrix for data of size $n$, which requires $O(n^3)$ operations and $O(n^2)$ memory in general(see \href{https://chenyw68.github.io/Literature/[2012]Advances and Challenges in Space-time Modelling of Natural Events.pdf}{\cite{porcu2012advances}}, \href{https://chenyw68.github.io/Literature/[2012]A full scale approximation of covariance functions for large spatial data sets.pdf}{\cite{sang2012full}}, \href{https://chenyw68.github.io/Literature/[2018]A case study competition among methods for analyzing large spatial data.pdf}{\cite{heaton2019case}}, \href{https://chenyw68.github.io/Literature/[2020]A Fused Gaussian Process Model for Very Large Spatial Data.pdf}{\cite{ma2020fused}}).

\section{Several approaches to overcome this large matrix problem}
Solutions to this computational intractability focus on model development, the design of efficient and parallel algorithms, and the improvement and efficient use of modern computing platforms (e.g. Using \href{https://hpc.niasra.uow.edu.au/azm/Spatial_GPUs.html}{TensorFlow} and  \href{https://github.com/cdeterman/gpuR}{GPU}).


Model:
\begin{itemize}
  \item [1)] Conditional distributions: \href{http://www.stat.uchicago.edu/~stein/}{Michael L. Stein} (The restricted likelihood, \href{https://chenyw68.github.io/Literature/[2004]Approximating likelihoods for large spatial data sets.pdf}{2004});
  \item [2)] Sparse covariance by tapering method: \href{http://user.math.uzh.ch/furrer/}{Reinhard Furrer}(\href{https://chenyw68.github.io/Literature/[2006]Covariance tapering for interpolation of large spatial datasets.pdf}{2006});
       \href{https://dnychka.github.io/}{Douglas Nychka}(bias correction, \href{https://chenyw68.github.io/Literature/[2008]Covariance Tapering for Likelihood Based Estimation in Large Spatial Data Sets.pdf}{2008}); \href{http://www.stat.uchicago.edu/~stein/}{Michael L. Stein}(Statistical Properties, \href{https://chenyw68.github.io/Literature/[2013]Statistical Properties of Covariance Tapers.pdf}{2013});
  \item [3)] Low-Rank methods: \href{https://niasra.uow.edu.au/cei/people/UOW202822.html}{Noel Cressie} (FRK, \href{https://chenyw68.github.io/Literature/[2008]Fixed rank kriging for very large spatial data sets.pdf}{2008}),
  \href{https://ph.ucla.edu/faculty/banerjee}{Sudipto Banerjee} (Predictive process, \href{https://chenyw68.github.io/Literature/[2008]Gaussian predictive process models for large spatial data sets.pdf}{2008});
  \item [4)] Sparse precision: \href{https://dnychka.github.io/}{Douglas Nychka}(lattice kriging, \href{https://chenyw68.github.io/Literature/[2015]A Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets.pdf}{2015}) by basis-function; \href{https://sites.google.com/view/katzfuss/}{Matthias Katfuss}(multiresolution approximation, \href{https://chenyw68.github.io/Literature/[2017]A Multi-Resolution Approximation for Massive Spatial Datasets.pdf}{2017}
) by basis-function;
   \href{https://www.maths.ed.ac.uk/~flindgre/}{Finn Lindgren} (GRMF Approximations, \href{https://chenyw68.github.io/Literature/[2011]An explicit link between GF and GMRFs the SPDE approach.pdf}{2011}) by SPDE;
   \href{http://abhidatta.com/}{Abhi Datta} (NNGP, \href{https://chenyw68.github.io/Literature/[2016]Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.pdf}{2016a},
\href{https://chenyw68.github.io/Literature/[2016]On nearest neighbor Gaussian process models for massive spatial data.pdf}{2016b},
\href{https://chenyw68.github.io/Literature/[2016]Nonseparable dynamic NNGP models for large spatio-temporal data.pdf}{2016c},
\href{https://chenyw68.github.io/Literature/[2019]Efficient algorithms for bayesian nearest neighbor gaussian processes.pdf}{2019}) by conditional distributions;
  \item [5)] Spectral method: \href{https://scholar.google.com/citations?user=xPuzEj8AAAAJ&hl=zh-CN}{Montserrat Fuentes} and \href{http://guinness.cals.cornell.edu/}{Joe Guinness}( Circulant embedding, \href{https://chenyw68.github.io/Literature/[2017]Circulant Embedding of Approximate Covariances for Inference From Gaussian Data on Large Lattices.pdf}{2017} and Periodic Embeddings,  \href{https://chenyw68.github.io/Literature/[2007]Approximate Likelihood for Large Irregularly Spaced Spatial Data.pdf}{2007},  \href{https://chenyw68.github.io/Literature/[2020]Spectral Density Estimation for Random Fields via Periodic Embeddings.pdf}{2019}).
  \item [6)] Discrete process convolutions. \href{https://scholar.google.com/citations?user=0FDdexMAAAAJ&hl=zh-CN}{Dave Higdon}(\href{https://chenyw68.github.io/Literature/[2002]Space and Space-Time Modeling Using Process Convolutions.pdf}{2002});
      \href{https://users.soe.ucsc.edu/~bruno/}{Bruno Sansó}(\href{https://chenyw68.github.io/Literature/[2009]A Spatio Temporal Model for Mean Anomaly and Trend Fields of North Atlantic Sea Surface Temperature.pdf}{2009}); \href{https://scholar.google.com/citations?user=qKdSQUsAAAAJ&hl=en}{Francky Fouedjio}(\href{https://chenyw68.github.io/Literature/[2016]A generalized convolution model and estimation for non-stationary random functions.pdf}{2016}). 
\end{itemize}


Algorithms and platforms:
\begin{itemize}
  \item [1)] INLA: \href{https://cemse.kaust.edu.sa/bayescomp}{Haavard Rue}.
  \item [2)] Parallel algorithm: \href{https://sites.google.com/view/katzfuss/}{Matthias Katfuss}.
  \item [3)] TensorFlow: \href{https://niasra.uow.edu.au/cei/people/UOW202823.html}{Andrew Zammit-Mangion}.
\end{itemize}

Other scholars:\href{https://www.stat.missouri.edu/people/wikle}{Christopher K. Wikle},
\href{https://bobby.gramacy.com/research/}{Robert B. Gramacy},
\href{https://www.counterpointstat.com/bradleycarlin.html}{Bradley P. Carlin},
\href{https://gufaculty360.georgetown.edu/s/contact/00336000014SrlEAAS/jonathan-stroud}{Jonathan R. Stroud},
\href{https://www.bus.miami.edu/thought-leadership/faculty/management-science/guan.html}{YongTao Guan},
\href{http://www2.stat.duke.edu/~alan/}{Alan E. Gelfand},
\href{https://www.lancaster.ac.uk/staff/diggle/}{Peter J. Diggle},
\href{https://www.stat.tamu.edu/~huiyan/}{Huiyan  Sang},
\href{https://www.canr.msu.edu/people/andrew_o_finley}{Andrw Finley},
\href{https://mheaton.byu.edu/docs/research.html}{Matthew J. Heaton}, 
\href{https://www.kaust.edu.sa/en/study/faculty/ying-sun}{Ying Sun},
\href{https://scholar.google.com/citations?user=dbFZvbwAAAAJ&hl=en}{Furong Sun}.

From an \textbf{approximation} point of view, the approximation of the likelihood in either the spatial or spectral domain is another solution to overcome computational obstacles.

\subsection{Likelihood approximations in the spatial domain by low rank methods}
\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
The reduced rank based methods usually fail to accurately capture the local, small scale dependence structure \href{https://chenyw68.github.io/Literature/[2012]A full scale approximation of covariance functions for large spatial data sets.pdf}{\citep{sang2012full}}. 
%The subsequent multiresolution (MR) methods (e.g. LatticeKrig, Multiresolution Approximations) compensates for this problem to some extent.
\subsubsection{\href{https://chenyw68.github.io/Literature/[2017]An R Package for Spatial and Spatio-Temporal Prediction with Large Datasets(FRK).pdf}{Fixed Rank Kriging}}
FRK (\href{https://chenyw68.github.io/Literature/[2006]Spatial prediction for massive datasets.pdf}{\citep{cressie2006spatial}}, \href{https://chenyw68.github.io/Literature/[2008]Fixed rank kriging for very large spatial data sets.pdf}{\citep{cressie2008fixed}}) aims to approximate the spatial process $w(s)$ in (\ref{model}) by a linear combination of $r$ ($<< n$) basis functions which ensures that all estimation and prediction equations only contain inverses of matrices of size $r \times r$.
\subsubsection{\href{https://chenyw68.github.io/Literature/[2015]spBayes for Large Spatio-Temporal Data Models.pdf}{Gaussian Predictive Processes} (\href{https://chenyw68.github.io/Literature/[2020]Bayesian spatially varying coefficient models in the spBayes R package.pdf}{GPP})}
With regard to the challenge of computational cost on covariance matrices, \href{https://chenyw68.github.io/Literature/[2008]Gaussian predictive process models for large spatial data sets.pdf}{\citet{banerjee2008gaussian}} proposed a class of models based on the idea of a spatial predictive process which is motivated by Kriging equation in (\ref{Kriging}). The predictive process projects the original process, $w(s)$ in (\ref{model}), onto a subspace generated by realizations of the original process at a specified set of locations (or knots). The approach is in the same spirit as process modeling approaches using basis functions and kernel convolutions, that is, specifications which attempt to facilitate computations through lower dimensional process representations.

\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
\begin{itemize}
 \item [1)]One advertised advantage of using the GPP approach as opposed to \href{https://chenyw68.github.io/Literature/[2017]FRK_intro.pdf}{FRK} or \href{https://chenyw68.github.io/Literature/[2014]LatticeKrig A multi-resolution spatial model for large data.pdf}{LatticeKrig} is that the GPP basis functions are completely determined by the choice of covariance function $C(\cdot, \cdot)$, and note that the subsequent \href{https://chenyw68.github.io/Literature/[2017]A Multi-Resolution Approximation for Massive Spatial Datasets.pdf}{Multiresolution approximations} is in step with GPP in this regard.
\item [2)]At the same time, however, when $C(\cdot, \cdot)$ is governed by unknown parameters (which is nearly always the case) the GPP basis functions need to be calculated iteratively rather than once as in FRK or LatticeKrig which will subsequently increase computation time.
\end{itemize}

%\href{https://chenyw68.github.io/Literature/[2012]Advances and Challenges in Space-time Modelling of Natural Events.pdf}{\citep{porcu2012advances}}

%\href{https://user.math.uzh.ch/furrer/software/KriSp/}{Tapering}


\subsection{Likelihood approximations in the spatial domain by sparse covariance methods}
\subsubsection{\href{https://user.math.uzh.ch/furrer/software/KriSp/}{Tapering}}
Including \href{https://chenyw68.github.io/Literature/[2008]Covariance Tapering for Likelihood Based Estimation in Large Spatial Data Sets.pdf}{tapering for estimation} (e.g. approximation log-likelihood function by a tapered covariance. See \href{https://chenyw68.github.io/Literature/[2008]Covariance Tapering for Likelihood Based Estimation in Large Spatial Data Sets.pdf}{\cite{kaufman2008covariance}}) and \href{https://chenyw68.github.io/Literature/[2006]Covariance tapering for interpolation of large spatial datasets.pdf}{tapering for Kriging} (or for interpolation or for prediction, e.g. approximation kriging equation by replacing the original covariance by a tapered version. See \href{https://chenyw68.github.io/Literature/[2006]Covariance tapering for interpolation of large spatial datasets.pdf}{\cite{furrer2006covariance}} and some R packages: \href{https://chenyw68.github.io/Literature/[2010]spam-A Sparse Matrix R Package with Emphasis on MCMC Methods for Gaussian Markov Random Fields.pdf}{spam}, 2010; \href{https://chenyw68.github.io/Literature/[2006]KriSp.pdff}{KriSp}, 2006; \href{https://chenyw68.github.io/Literature/[2020]fields.pdf}{fields}, 2017.

\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
The covariance tapering has shown great computational gains, but it also has its own drawbacks. 
\begin{itemize}
 \item [1)]The covariance tapering may not be effective in accounting for \textbf{spatial dependence with long range}.
 \item [2)]The accuracy of the tapering approximation for \textbf{nonstationary problems} remains an open question, and the application of tapering techniques to \textbf{multivariate random fields} remains to be explored due to the lack of flexible compactly supported cross-covariance functions, see \href{https://chenyw68.github.io/Literature/[2012]Advances and Challenges in Space-time Modelling of Natural Events.pdf}{\citet{porcu2012advances}}.
\end{itemize}
\subsubsection{Spatial Partitioning}
By dividing region $\boldsymbol{D}$ into $m$ disjoint subregions ($d = 1, 2, \cdots, m$),  and then the modeling approach based on spatial
partitioning is to again assume the model in (\ref{model2}) but take on the assumption of independence between observations across subregions. 
\begin{equation}
\begin{aligned}
\boldsymbol{y}_d = \boldsymbol{\beta}\boldsymbol{X}_d + \boldsymbol{H}_d\boldsymbol{w} + \boldsymbol{\xi}_d + \boldsymbol{\epsilon}_d
\end{aligned} \label{model2}
\end{equation}
where $\boldsymbol{H}_d$ is a matrix of spatial basis function for subregions $d$.

\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
\begin{itemize}
 \item [1)] Notice that, in (\ref{model2}) each subregion shares common $\boldsymbol{\beta}$ and $\boldsymbol{w}$ parameters which allows smoothing across subregions in spite of the independence assumption. Further, the assumption of independence across subregions effectively creates a block-diagonal structure for $\boldsymbol{\Sigma}$ and allows the likelihood to be computed in parallel (with one node per subregion), thereby facilitating computation.
 \item [2)] The key to implementing the spatial partitioning approach is the choice of partition and the literature is replete with various options. 
  A priori methods to define the spatial
partitioning include partitioning the region into equal areas (\href{https://chenyw68.github.io/Literature/[2011]Covariance approximation for large multivariate spatial data sets.pdf}{\citep{sang2011covariance}}), partitioning based on centroid clustering (\href{https://chenyw68.github.io/Literature/[2005]Analyzing nonstationary spatial data using piecewise Gaussian processes.pdf}{\citep{kim2005analyzing}}), hierarchical clustering based on spatial gradients (\href{https://chenyw68.github.io/Literature/[2017]Nonstationary Gaussian Process Models Using Spatial Hierarchical Clustering from Finite Differences.pdf}{\citep{heaton2017nonstationary}}). Alternatively, model-based approaches to spatial partitioning include treed regression(\href{https://chenyw68.github.io/Literature/[2014]Adaptive Bayesian Nonstationary Modeling for Large Spatial Datasets Using Covariance Approximations.pdf}{\citep{konomi2014adaptive}}) and mixture modeling (\href{https://chenyw68.github.io/Literature/[2014]A multivariate spatial mixture model for areal data.pdf}{\citet{neelon2014multivariate}}), but these approaches typically require more computation.
\end{itemize}


\subsection{Likelihood approximations in the spatial domain by sparse precision methods}
The sparse precision methods focus on basis function and conditional likelihood.
\subsubsection{\href{https://chenyw68.github.io/Literature/[2020]LatticeKrigVignette.pdf}{LatticeKrig}}
\href{https://chenyw68.github.io/Literature/[2015]A Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets.pdf}{\citep{nychka2015multiresolution}}
\subsubsection{Multiresolution Approximations}
 (see  \href{https://chenyw68.github.io/Literature/[2017-formal]Parallel inference for massive distributed spatial data using low-rank models.pdf
}{\citep{katzfuss2017parallel}} and  \href{https://chenyw68.github.io/Literature/[2017]A Multi-Resolution Approximation for Massive Spatial Datasets.pdf}{\citep{katzfuss2017multi}})
\subsubsection{SPDE/INLA}
The numerical factorization of the precision matrix using sparse matrix algorithms can be done at a typical cost of $O(n^{3/2})$ for two-dimensional GMRFs.

\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
The drawback of this approachis that we can only find the explicit form of GMRFs for those Gaussian random fields that have a Matérn covariance structure at certain integer smoothnesses.  they can be extended to model Matérn covariances on the sphere, nonstationary locally isotropic Gaussian random fields, Gaussian random fields with oscillating correlation functions, and non-isotropic fields.


\subsubsection{NNGP}
The nearest neighbor Gaussian process (\href{https://chenyw68.github.io/Literature/[2016]Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.pdf}{\cite{datta2016hierarchical}},
\href{https://chenyw68.github.io/Literature/[2016]Nonseparable dynamic NNGP models for large spatio-temporal data.pdf}{\cite{datta2016nonseparable}},
\href{https://chenyw68.github.io/Literature/[2016]Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.pdf}{\cite{finley2019efficient}}
) is defined from the conditional specification of the joint distribution of spatial process $w(s)$ in (\ref{model}), one forms of composite likelihoods which is motivated by Vecchia (1988) ideas.  The total flop counts is of the order $(n + k)m^3$,
where $m (\approx 20)$, NNGP is much faster than the full Gaussian model which requires $O(n^3 )$ flops.

Composite likelihoods which points out the difficulty in choosing conditioning sets and  evaluation of the approximation accuracy, but with the advent of the NNGP approach, the problem has been solved.

\subsection{Likelihood approximations in the spectral Domain: Circulant embedding and Periodic embeddings}
\textcolor[rgb]{1.00,0.00,1.00}{Comments:}
The spectral methods are computationally efficient by avoiding the calculation of determinants and can be easily adapted to model nonstationary processes as a mixture of independent stationary processes. [15] presented a version of Whittle’s approximation to the Gaussian
negative log-likelihood by introducing a lattice process which can be used to deal with irregularly spaced data. Additional computational savings were obtained by truncating the spectral representation of the lattice process. If $n$ is the total number of observations of the process $\boldsymbol{Y}$, m is lattice size, the calculation requires $O(m \log_2 m + n)$ operations rather than $O(n^3)$ for the exact likelihood of $\boldsymbol{Y}$.

However, they do not overcome the difficulty in prediction with massive data.

%----------------------------------------------------------------------------------------
%\bibliographystyle{achemso}
\bibliography{summary_spatial}
\end{document} 